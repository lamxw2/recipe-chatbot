{
  "": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 3
  },
  "ask_recipe": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6
  },
  "action_inform_healthy_eating_examples": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[{\"button_choice\": \"cancel\"}]{\"entity\": \"button_choice\", \"value\": \"cancel\"}": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_tellajoke": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[chicken](proteins)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4
  },
  "[{\"button_choice\": \"no\"}]{\"entity\": \"button_choice\", \"value\": \"no\"}": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[pork](proteins)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "chitchat": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "action_retrieve_recipes": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6
  },
  "thank": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "greet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_noproblem": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "ask_healthy_eating_basic": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "ask_for_a_joke": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[ice-cream](dairy)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[dairy](dairy)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "action_listen": {
    "precision": 0.85,
    "recall": 1.0,
    "f1-score": 0.9189189189189189,
    "support": 17
  },
  "utter_introduce": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "action_inform_healthy_eating_basic": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "[taco](dish_categories)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "None": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 1
  },
  "ask_healthy_eating_examples": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[breakfast](dish_categories)": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "action_button_choice": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 3
  },
  "utter_chitchat": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "[{\"button_choice\": \"yes\"}]{\"entity\": \"button_choice\", \"value\": \"yes\"}": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "micro avg": {
    "precision": 0.9508196721311475,
    "recall": 0.8923076923076924,
    "f1-score": 0.9206349206349206,
    "support": 65
  },
  "macro avg": {
    "precision": 0.8833333333333334,
    "recall": 0.8888888888888888,
    "f1-score": 0.8858858858858859,
    "support": 65
  },
  "weighted avg": {
    "precision": 0.8530769230769231,
    "recall": 0.8923076923076924,
    "f1-score": 0.8711018711018711,
    "support": 65
  },
  "conversation_accuracy": {
    "accuracy": 0.6363636363636364,
    "correct": 7,
    "total": 11
  }
}